<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Arts | 平凡的世界]]></title>
  <link href="http://linuxlsx.github.io/blog/categories/arts/atom.xml" rel="self"/>
  <link href="http://linuxlsx.github.io/"/>
  <updated>2019-05-05T21:08:17+08:00</updated>
  <id>http://linuxlsx.github.io/</id>
  <author>
    <name><![CDATA[linuxlsx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Arts-week-7]]></title>
    <link href="http://linuxlsx.github.io/blog/2019/05/05/arts-week-7/"/>
    <updated>2019-05-05T21:06:15+08:00</updated>
    <id>http://linuxlsx.github.io/blog/2019/05/05/arts-week-7</id>
    <content type="html"><![CDATA[<h2>Algorithm</h2>

<p>本周完成的算法题: <a href="https://leetcode.com/problems/wildcard-matching/">Wildcard Matching</a></p>

<p>题目要求:</p>

<pre><code>Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for '?' and '*'.

'?' Matches any single character.
'*' Matches any sequence of characters (including the empty sequence).
The matching should cover the entire input string (not partial).
</code></pre>

<!-- more -->


<p>本题可以通过动态规划解决。时间复杂度和空间复杂度都为O(mn)</p>

<pre><code class="Java">    public boolean isMatch(String s, String p) {

        boolean[][] match = new boolean[s.length() + 1][p.length() + 1];
        match[0][0] = true;

        // 初始化空串的匹配。
        for (int i = 1; i &lt;= p.length(); i++) {
            if (p.charAt(i - 1) == '*') {
                match[0][i] = match[0][i - 1];
            }
        }

        for (int i = 1; i &lt;= s.length(); i++) {
            for (int j = 1; j &lt;= p.length(); j++) {
                if (p.charAt(j - 1) != '*') {
                    match[i][j] = match[i - 1][j - 1] &amp;&amp; (p.charAt(j - 1) == '?' || s.charAt(i - 1) == p.charAt(j - 1));
                } else {
                    match[i][j] = match[i][j - 1] || match[i - 1][j];
                }
            }
        }
        return match[s.length()][p.length()];
    }
</code></pre>

<h2>Review</h2>

<p>本周Review <a href="https://medium.com/@vaibhav0109/how-to-build-a-tiny-url-service-that-scales-to-billions-f6fb5ea22e8c">How to build a Tiny URL service that scales to billions?</a></p>

<p>本文主要介绍了在Twitter和微博类应用场景下，如何把长URL转成短URL的实践。作者的实现是<a href="https://tinyurl.com/">TinyUrl</a></p>

<p>目标是：</p>

<pre><code>Creating a unique, repeatable identifier for a URL
</code></pre>

<p>直接使用Hash算法可不可以？作者的建议是不推荐，主要有以下几个缺点:</p>

<ul>
<li>长度。大多数常规的Hash算法都会产出长度比较大的字符串(32位，128位)，这个不符合短URL的设计目标</li>
<li>唯一性。Hash算法在本质上并不唯一(存在重复可能)，所以需要考虑重复后的替代方案</li>
<li>查找。使用Hash后的字符作为数据的Key，在查询上并不友好。</li>
</ul>


<h3>算法实现</h3>

<p>在TinyUrl中，使用了名为Base62的算法，就是使用[0-9 a-z A-Z]62个字符来表示一个URL。使用62个字符且字符串长度为7，8，9，10，11时可以表示的URL数量为:</p>

<pre><code>62⁷ = 3,521,614,606,208 urls
62⁸ = 218,340,105,584,896 urls
62⁹ = 13,537,086,546,263,552 urls
62¹⁰ = 839,299,365,868,340,224 urls
62¹¹ = 52,036,560,683,837,093,888 urls
</code></pre>

<p>使用7个字符可以表示3500亿，使用8个字符可以表示218万亿个URL。以下是算法的实现部分：首先将要转换的URL生成一个对应的整数(seed)，然后根据这个seed来生成短URL.</p>

<pre><code class="Java">private static final char[] corpus   = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789".toCharArray();
/*
* 通过输入一个数字种子，返回该种子对应的Base62字符串。种子不同的情况下可以保证不会重复
*/
public static final String getBase62From10(final long seed)
{
  String number = seed + "";
  char[] buf = new char[number.length()];
  int charPos = number.length() - 1;
  BigInteger bigIntegerNumber = new BigInteger(number);
  BigInteger radix = BigInteger.valueOf(62);

  while (bigIntegerNumber.compareTo(radix) &gt;= 0)
  {
   buf[charPos--] = corpus[bigIntegerNumber.mod(radix).intValue()];
   bigIntegerNumber = bigIntegerNumber.divide(radix);
  }
  buf[charPos] = corpus[bigIntegerNumber.intValue()];
  return new String(buf, charPos, (number.length() - charPos));
}
/**
* 将一个62进制的整数转换为10进制的整数。
*/
public static final String getBase10From62(final long longNumber)
{
  String number = longNumber + "";
  BigInteger value = BigInteger.ZERO;
  for (char c : number.toCharArray())
  {
    value = value.multiply(BigInteger.valueOf(62)); 
    if ('0' &lt;= c &amp;&amp; c &lt;= '9')
    {
      value = value.add(BigInteger.valueOf(c - '0'));
    }
    if ('a' &lt;= c &amp;&amp; c &lt;= 'z')
    {
      value = value.add(BigInteger.valueOf(c - 'a' + 10));
    }
    if ('A' &lt;= c &amp;&amp; c &lt;= 'Z')
    {
      value = value.add(BigInteger.valueOf(c - 'A' + 36));
    }
   }
   return value.toString();
}
</code></pre>

<h3>设计</h3>

<p>简单的方案可以通过数据表来实现，表可能的字段有：</p>

<ul>
<li>id (DB generated sequence ID)</li>
<li>original_url      原来的链接</li>
<li>shorten_url       短连接</li>
<li>creation_date     创建时间</li>
<li>expiration_date   过期时间</li>
</ul>


<p>交互流程如下图：</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/TinyUrl-Flow-1.png?x-oss-process=style/black" alt="TinyUrl-Flow-1" /></p>

<p>这个简单方案有两个缺点：</p>

<ol>
<li>需要两次数据库交互。在高并发场景下会对数据库造成压力</li>
<li>当数据量达到一定数据级后迁移到多个表，数据库的自增ID会有重复，导致重复的短域名产生。</li>
</ol>


<p>现在来改进下这个方案，需要修改下表结构同时数据库操作只需要一次insert即可。新的字段如下：</p>

<ul>
<li>id_hash           通过Base62算法生成的字符串作为主键</li>
<li>original_url      原来的链接</li>
<li>shorten_url       短连接</li>
<li>creation_date     创建时间</li>
<li>expiration_date   过期时间</li>
</ul>


<p>在新方案中使用Base62产生的字符串做为主键而不是使用数据库自增ID。这样要求引入一个能够提供唯一ID的服务(可以利用Redis的自增特性)。新的方案交互流程如下：</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/TinyUrl-Flow-2.png?x-oss-process=style/black" alt="TinyUrl-Flow-2" /></p>

<h3>其他</h3>

<h4>清除过期数据</h4>

<ul>
<li>当用户访问过期URL的时候，清理URL并返回错误给用户</li>
<li>使用定时任务来主动清理(在系统低峰时执行)</li>
</ul>


<h4>安全性</h4>

<ul>
<li>由于<code>id_hash</code>是基于一个整数计算出来的，那么用户可以通过<code>id+1</code>的方式来获取出下一个TinyUrl。如果是私密数据的话这样的操作是不允许的。可以通过在url之后增加随机数的方式来解决这个问题，但是相应的会增加url的长度，这个是需要权衡的。</li>
</ul>


<h2>Tip</h2>

<p>本周做算法题的时候卡了好久，通过查询相关资料才了解到应该使用动态规划来解。在网上搜寻了不少资料，文字版总感觉某些地方绕不过来。终于在油管上找到了一个讲解这个问题的视频，讲的清晰易懂，分享给大家。
<a href="https://www.youtube.com/watch?v=3ZDZ-N0EPV0">https://www.youtube.com/watch?v=3ZDZ-N0EPV0</a></p>

<h2>Share</h2>

<p>本周没有自己写文章，结合自己最近的工作主要是知识图谱。所以给大家分享一份由清华大学整理的关于知识图谱的介绍文档：<a href="https://www.aminer.cn/research_report/5c3d5a8709e961951592a49d?download=true&amp;pathname=knowledgegraph.pdf">2019年第二期《人工智能之知识图谱》</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Arts-week-six]]></title>
    <link href="http://linuxlsx.github.io/blog/2019/04/28/arts-week-six/"/>
    <updated>2019-04-28T21:50:10+08:00</updated>
    <id>http://linuxlsx.github.io/blog/2019/04/28/arts-week-six</id>
    <content type="html"><![CDATA[<h2>Algorithm</h2>

<p>本周完成的算法题: <a href="https://leetcode.com/problems/permutations/">Permutations</a></p>

<p>题目要求:</p>

<pre><code>给定一个不重复的整数数组，求所有可能的排列
</code></pre>

<!-- more -->


<p>本题可以通过回溯法来解决，需要注意排除重复添加元素的情况</p>

<pre><code class="Java">    public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) {
        List&lt;List&lt;Integer&gt;&gt; lists = new ArrayList&lt;&gt;();
        backtrack(nums, lists, new ArrayList&lt;&gt;());

        return lists;
    }

    private void backtrack(int[] nums, List&lt;List&lt;Integer&gt;&gt; lists, List&lt;Integer&gt; stack) {
        if (stack.size() == nums.length) {
            lists.add(new ArrayList&lt;&gt;(stack));
        } else {
            for (int i = 0; i &lt; nums.length; i++) {
                if (stack.contains(nums[i])){
                    continue;
                }
                stack.add(nums[i]);
                backtrack(nums, lists, stack);
                stack.remove(stack.size() - 1);
            }
        }
    }
</code></pre>

<h2>Review</h2>

<p>本周Review <a href="https://medium.com/javarevisited/java-8-parallel-stream-java2blog-e1254e593763">Introduction to Java 8 Parallel Stream</a></p>

<p>本文主要讲述了什么时候应该使用<code>Parallel Stream</code>。文中提到<code>Parallel Stream</code>要比<code>Sequential Stream</code> 花费更多的时间在线程协调上，所以在使用<code>Parallel Stream</code> 之前你需要考虑一下的几个因素：</p>

<ul>
<li>要处理的数据集要大</li>
<li>Java使用<code>ForkJoinPool</code>来达到并行的目的。所以源头的集合要可切分(Splitable)，ArrayList 要比 LinkedList 要好</li>
<li>现在确实有性能上的问题</li>
<li>要处理好数据共享的问题</li>
</ul>


<p>通过<code>NQ</code>模型(来自 Brian Goetz)可以衡量是否要做并行化：</p>

<pre><code>N x Q &gt; 10000
</code></pre>

<p>其中：</p>

<p>N = 数据集中的条目数量（number of items in the dataset）
Q = 每个条目需要的工作（amount of work per item）</p>

<p>解释起来就是：如果数据集多但是每个条目需要工作少，或者数据集不多但是每个条目需要的工作多，你就可以通过并行化来获的性能上的优化。</p>

<h2>Tip</h2>

<p>上面Review提到<code>Parallel Stream</code>使用到了<code>ForkJoinPool</code>，这中间一个小细节需要注意：</p>

<p>默认情况下所有的<code>Parallel Stream</code>都是共用<code>ForkJoinPool.common</code>这个池子，所以如果Stream后的任务比较耗时(比如有网络请求等)，会堵塞住整个池子，导致其他的Stream也会被影响到。</p>

<p>如果不想被其他的影响也不想影响其他，可以通过自定有的池子来执行。以下是一个示例:</p>

<pre><code class="Java">    long firstNum = 1;
    long lastNum = 1000000;

    List&lt;Long&gt; aList = LongStream.rangeClosed(firstNum, lastNum).boxed()
      .collect(Collectors.toList());

    ForkJoinPool customThreadPool = new ForkJoinPool(4);
    long actualTotal = customThreadPool.submit(
      () -&gt; aList.parallelStream().reduce(0L, Long::sum)).get();
</code></pre>

<p>参考:</p>

<p><a href="https://dzone.com/articles/think-twice-using-java-8">Think Twice Before Using Java 8 Parallel Streams</a></p>

<p><a href="https://stackoverflow.com/questions/21163108/custom-thread-pool-in-java-8-parallel-stream">Custom thread pool in Java 8 parallel stream</a></p>

<h2>Share</h2>

<p>在项目中碰到了一个因为超大规模的数据(30亿)导致原本执行挺快的模块连续执行超过1天也没有执行完，而且也看不到执行完的希望。于是开始排查是哪里出现了问题。</p>

<h3>背景描述</h3>

<p>业务方基于业务需要配置了约11000条规则，每个规则都是对某个实体的属性判断，最终保存到系统是类似如下的表达式:</p>

<pre><code>"1":"((product_word = '育儿书') OR (product_word = '故事书' AND press = '故事会') OR (product_word = '绘画' AND author = '绘画大师') OR (product_word = '益智游戏类图书') OR (product_word = '儿童读物') OR (product_word = '启蒙认知类图书'))
</code></pre>

<p>我们需要给出每个规则会命中哪些数据，或者说每条数据会命中哪些规则。</p>

<h3>原始方案</h3>

<p>在最开始的实现中，我们是把规则当做一个求值表达式，通过内部的一个DSL引擎对每个规则来做判断，如果返回<code>true</code>就说明命中规则，如果是<code>false</code>说明没有命中规则。</p>

<p>单条规则每次求值时间在<strong>0.04ms</strong>，11000条规则每次执行需要<strong>44ms</strong>左右，在个人机器上测试10w条数据需要大概<strong>4000s</strong>，这个时间完全不可接受的。</p>

<h3>分析问题</h3>

<p>经过数据分析发现就是无效的规则执行太多了，在数据确定的情况下，很多规则是肯定不会命中的。比如有一条数据是这样的:</p>

<pre><code>product_word = 育儿书
press = 中信出版社
author = 小蘑菇
</code></pre>

<p>那它实际需要执行的规则是那些规则条件中包含了 育儿书、中信出版社、小蘑菇的规则，至于其他不包含这些的规则肯定是不会命中的，也就无需执行了。</p>

<p>所以解决方案是：预先为属性、属性值、规则之间建立类似倒排索引的结构，通过属性值来查找可能要执行的规则，然后再执行。</p>

<h3>解决方案</h3>

<p>在解决方案细节中我们需要额外考虑的一点是操作符的问题。对于不同的操作符需要做不同的设计:</p>

<ul>
<li>对于 <strong>Equal(=)</strong> 是可以直接通过值来获取到所有有关系的规则。</li>
<li><code>&gt;</code> <code>&gt;=</code> <code>&lt;</code> <code>&lt;=</code> <code>in</code> <code>not in</code> 等操作符需要做进步一求值才行，但是考虑这些操作符总体使用数量很少，那就直接把这些规则都加到候选规则集中，由求值引擎统一执行即可。</li>
</ul>


<p>假设我们现在有三个属性 product_word，press，author 以及两个操作符 <code>=</code> <code>in</code>，那么最终构成的索引结构如下图：</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E%E4%B8%8E%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E%E8%9E%8D%E5%90%88-%E6%B1%82%E5%80%BC%E4%BC%98%E5%8C%96.png?x-oss-process=style/black" alt="倒排索引" /></p>

<p>上下层级之间实现都是一个HashMap结构，可以快速的定位到目标数据。有了这个结构后再来看看当输入以下这条数据的时候，需要执行哪些规则：</p>

<pre><code>product_word = 育儿书
press = 中信出版社
author = 小蘑菇
</code></pre>

<ul>
<li>通过 <code>product_word = 育儿书</code> 可以找到规则 <code>R:1</code></li>
<li><code>product_word</code> 还有一个 <code>in</code> 操作符，根据前面提到的原则把这个操作符关联的规则全部加入候选集。现在的规则包含  <code>R:1</code> <code>R:2</code></li>
<li>通过 <code>press = 中信出版社</code> 可以找到规则 <code>R:1</code></li>
<li>通过 <code>author = 小蘑菇</code> 可以找到规则 <code>R:1</code></li>
</ul>


<p>最后要执行的规则集为 <code>[R:1, R:2]</code>，比原来的方案要少执行了规则<code>R:3</code>。</p>

<p>使用这个方案后实际上一条数据最多可能需要执行的规则为<code>100条</code>，更多的少于<code>10条</code>。在本地测试同样执行10w条数据，只需要<strong>27s</strong>，性能提升将近<strong>150倍</strong>。</p>

<h3>总结</h3>

<p>在大规模数据处理中，尽可能的<strong>减少输入数据量</strong>，<strong>减少关键步骤执行次数和时间</strong>是优化性能的两大利器。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Arts-week-five]]></title>
    <link href="http://linuxlsx.github.io/blog/2019/04/22/arts-week-five/"/>
    <updated>2019-04-22T00:58:05+08:00</updated>
    <id>http://linuxlsx.github.io/blog/2019/04/22/arts-week-five</id>
    <content type="html"><![CDATA[<h2>Algorithm</h2>

<p>本周完成的算法题: <a href="https://leetcode.com/problems/trapping-rain-water/">Trapping Rain Water</a></p>

<p>题目要求:</p>

<pre><code>给定一个n个大于等于0的整数，每个整数代表一个宽度为1的竖条的高度。计算其组成的图形可以在雨后保留多少单位的水
</code></pre>

<p><img src="https://assets.leetcode.com/uploads/2018/10/22/rainwatertrap.png" alt="rainwatertrap.png" /></p>

<!-- more -->


<p>这个是图代表数组<code>[0,1,0,2,1,0,1,3,2,1,2,1]</code>，在这个情况下，可以保留6个单位的雨水。</p>

<p><strong>分析:</strong></p>

<p>这个题我原始解决思路是尝试从左往右找到最大的一个蓄水区间。如下图(a)所示: 条件是 <code>right &gt; left</code>，这样可以直接计算出<strong>left</strong>和<strong>right</strong>之间的蓄水量。当然还有一种情况如图(b)所示，<code>left</code>是剩下所有中最高的，那么我们需要找到右侧中最大的一个，把它当做<strong>right</strong>和<strong>left</strong>计算水数量。然后让<code>left = right</code>继续迭代。</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/rainwatertrap.png?x-oss-process=style/black" alt="rainwatertrapv2.png" /></p>

<p>算法复杂的：在正常情况下是<code>O(N)</code>，最坏情况是<code>O(N^2)</code>，这个会在数组是倒序有序的情况下出现，因为每次都要往右循环到底才能确定<strong>right</strong>的位置。</p>

<pre><code class="Java">
    /**
    *  runtime 1ms, beats 99%
    */
    public int trap(int[] height) {

        if (height.length &lt; 3) {
            return 0;
        }

        int total = 0;
        int left = 0;
        int maxLessThanLeftIndex = 0;
        int maxLessThanLeftValue = 0;
        int solid = 0;

        while (left &lt; height.length - 2) {

            boolean cal = false;
            for (int i = left + 1; i &lt; height.length; i++) {
                if (height[left] &gt;= height[i]) {
                    solid += height[i];

                    //计算第二大的数，以便后续回溯使用
                    if (maxLessThanLeftValue &lt;= height[i]) {
                        maxLessThanLeftIndex = i;
                        maxLessThanLeftValue = height[i];
                    }
                } else {
                    //在这里说明要计算蓄水了
                    total += (Math.min(height[left], height[i]) * (i - left - 1) - solid);
                    left = i;
                    cal = true;
                    break;
                }
            }

            if (!cal) {
                //到这里说明左边界是最大的值，需要使用第二大的数来做右边界。
                solid = 0;
                for (int i = left + 1; i &lt; maxLessThanLeftIndex; i++) {
                    solid += height[i];
                }
                total += (Math.min(height[left], height[maxLessThanLeftIndex]) * (maxLessThanLeftIndex - left - 1) - solid);

                left = maxLessThanLeftIndex;
            }

            maxLessThanLeftIndex = 0;
            maxLessThanLeftValue = 0;
            solid = 0;
        }


        return total;
    }
</code></pre>

<p>在这个题的Solution中有一个更简洁的双指针版本，算法复杂度O(N)。其思想是单独为每个位置计算它的蓄水量，一个位置的蓄水量的大小是有它两侧最高点中较小的一个减去该位置的高度决定的(公式  = <code>min(left_max, right_max) - height</code>) ，比如我们在计算位置3的蓄水量时，它的left_max是位置1，right_max是位置10，根据公式位置3的蓄水量=<code>left_max - height</code></p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/rainwatertrapv2.png?x-oss-process=style/black" alt="rainwatertrapv2" /></p>

<p>代码更简单:</p>

<pre><code class="Java">    int trap2Points(int[] height) {
        int left = 0, right = height.length - 1;
        int ans = 0;
        int left_max = 0, right_max = 0;
        while (left &lt; right) {
            if (height[left] &lt; height[right]) {
                if (height[left] &gt;= left_max) {
                    left_max = height[left];
                } else {
                    ans += (left_max - height[left]);
                }
                ++left;
            } else {
                if (height[right] &gt;= right_max) {
                    right_max = height[right];
                } else {
                    ans += (right_max - height[right]);
                }
                --right;
            }
        }
        return ans;
    }
</code></pre>

<h2>Review</h2>

<p>本周Review <a href="https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb">Avoiding Double Payments in a Distributed Payments System</a></p>

<p>本文介绍了Aribnb支付团队是如何分析和解决重复支付的问题。</p>

<h3>背景介绍</h3>

<p>Aribnb内部服务架构像SOA转型，这就导致原本在一个系统或者大服务里面完成的功能(比如下单和支付)，会被拆分成独立的服务。这样能够让开发人员更加专注且快速迭代，但是也引入了如何保证数据一致性的问题。</p>

<p>为了解决分布式一致性的问题，文中提到了三种解决方案：</p>

<ul>
<li>read repair. 在读取求的时候进行一致性的校验和补偿。<a href="https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsRepairNodesReadRepair.html">参考</a>。

<ul>
<li>优点：客户端简单，无需关注一致性逻辑</li>
<li>缺点：服务端需要额外的逻辑来保证一致性，会降低读的吞吐量。</li>
</ul>
</li>
<li>write repair. 客户端通过重试的方式来保证数据一致。

<ul>
<li>优点：服务端无需额外的逻辑保证一致性。</li>
<li>缺点：客户端需要维护一套重试的逻辑。服务端需要保证重试请求的幂等性</li>
</ul>
</li>
<li>asynchronous repair. 通过后台程序，定时任务等检查数据一致性，并通知客户端结果。

<ul>
<li>优点：与主逻辑解耦</li>
<li>缺点：会有一定的延迟性</li>
</ul>
</li>
</ul>


<p>在实践上可以把<code>asynchronous repair</code>作为read repair和write repair的第二道防线。</p>

<h3>幂等性[idempotency]</h3>

<p>幂等性是下单&amp;支付这类分布式应用中最基本的要求，必须保证每次请求<code>有且仅被处理一次</code>。如果不这样的话就会出现重复扣款的情况。同时幂等性允许一次操作多次重试，并且得到一致的结果。</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/1_bft4XyiErJha_uhbGLL6-w.png" alt="幂等性" /></p>

<h3>问题难点描述</h3>

<ul>
<li>需要设计一个通用支持多个支付服务的幂等性解决方案。</li>
<li>数据一致性不能妥协。这个会影响公司的形象</li>
<li>低延迟。需要分布式部署。</li>
<li>简单易用。业务开发不需要了解底层实现细节</li>
</ul>


<h3>解决方案</h3>

<p>Airbnb支付团队实现了一个通用幂等库(<code>general-purpose idempotency library</code>)，取得名字叫<a href="https://zh.wikipedia.org/wiki/%E4%BF%84%E8%80%B3%E7%94%AB%E6%96%AF">Orpheus</a>。(发现国内外取名都喜欢用希腊神话故事里面的任务)</p>

<p>这个库可以提供低延迟，并且与上下游良好隔离的实现。从整体来看，主要有以下几个概念：</p>

<ul>
<li><strong>idempotency key</strong> 通过一个idempotency key来代表一次幂等的请求。</li>
<li><strong>sharded master</strong> 数据的读和写都走数据库Master节点</li>
<li><strong>Database transactions</strong> 使用<code>Java Lambdas</code>将分布在不同代码中的逻辑组合到一起保证原子性。</li>
</ul>


<p>Orpheus将一次请求划分为三个阶段: Pre-RPC, RPS和Post-RPC:</p>

<ul>
<li><strong>Pre-RPC</strong> 将请求详情持久化到数据库</li>
<li><strong>RPC</strong> 执行业务请求</li>
<li><strong>Post-RPC</strong> 将请求结果持久化的数据库。包括: 成功、失败以及是否可以重试等。</li>
</ul>


<p>这里有两条基本原则：</p>

<ol>
<li>在Pre-RPC和Post-RPC阶段不会有业务逻辑</li>
<li>在RPC阶段不会有数据库操作</li>
</ol>


<p>通过这两个原则将网络通信和数据库操作隔离开来，避免处理因为网络带来的各种不确定的问题，比如超时等。同时为了保证一致性，框架将Pre-RPC和Post-RPC放到同一个事物中执行，这样可以保证这两个阶段一定是从事成功或者失败。以下是一次请求的示意图:</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/1_EzRHQV1GaNO6fXg8NSQ2Cg.png" alt="请求流程图" /></p>

<h3>异常处理</h3>

<p>当出现异常时，框架需要明确告知客户端本次请求是可重试的还是不可重试的。Airbnb通过以下的策略将不同的异常划分为可重试或者不可重试.</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/1__q2kiqlR69N_Tybu37Px2Q.png" alt="异常处理" /></p>

<h3>客户端至关重要</h3>

<p>在本文的场景下，客户端需要一些智能。他需要有以下几项关键职责：</p>

<ol>
<li>为每个请求生成一个唯一的<code>Idempotency Key</code>,并且在重试的时候复用原来的key</li>
<li>请求之前持久化key</li>
<li>合适的处理成功请求</li>
<li>在不允许重试的时候要处理好请求压力</li>
<li>设定合适的重试策略</li>
</ol>


<h3>怎么选择一个 Idempotency Key</h3>

<p>有两种方式来生成一个Idempotency Key：</p>

<ol>
<li><strong>request-level</strong> 每次请求都使用一个随机且唯一的key。比如UUID，snowflake等</li>
<li><strong>entity-level</strong> 包含了业务含义的key，比request-level的要更加严格。可以通过实体模型的关键字段来生成一个明确的key. 比如支付操作<code>1234</code>只能产生一次退款操作，那么可以定义类似<code>payment-1234-refund</code>的key来保证唯一性。</li>
</ol>


<h3>每个请求都有可过期的租约(Expiring Lease)</h3>

<p>在请求处理之前需要先获取一个锁，拿到锁以后才可以继续处理数据。锁一般要有超时时间，当超时时间到了以后可以发起重试。</p>

<p>推荐锁的超时间要比RPC请求的超时时间长。否则可能会有重复请求。</p>

<h3>坚持使用数据库Master节点</h3>

<p>Orpheus通过数据来保存<code>Idempotency Key</code>等重要数据，为了保证数据一致性所有读写操作都在Master节点上执行。避免主从结构中因为数据复制可能出现的重复操作。以下是主从结构中可能出现的重复操作示意图:</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/1_ASFZE0BWYFYxokg1WUgnZw.png" alt="主从有问题" /></p>

<p>只用主节点就不会出现因为数据复制导致可能的重复操作:</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/1_FBLUYnSjxQ15uSdAisvriA.png" alt="只用主没问题" /></p>

<p>如果只用一个主节点的话，主节点就会成为整个系统的瓶颈。所以系统设计通过key分片的方式部署多个主节点，避免一个主节点过热的问题。</p>

<h3>总结</h3>

<p>想要做到数据一致性就会引入额外的复杂度。没有付出就没有回报。
Orpheus目前达到了5个9的一致性。</p>

<p>注意: Review中出现的图片均引用自<a href="https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb">Avoiding Double Payments in a Distributed Payments System</a>，版权归它所有。</p>

<h2>Tip</h2>

<p>本周分享一个线上排查问题时查看占用CPU高的前几个线程。</p>

<pre><code>ps -eL -o pid,%cpu,lwp | grep -i &lt;pid&gt; | sort -nr -k2 | awk '{printf("%s %s %x\n", $1,$2,$3)}' | head
</code></pre>

<h2>Share</h2>

<p><strong>Gson源码阅读(一)</strong></p>

<p><code>Gson</code>是开发中常用的<code>Json</code>库，使用起来非常简单。整个<a href="https://github.com/google/gson">工程的源码</a>不多，非常适合作为源码阅读的开胃菜。</p>

<p><code>Gson</code>的核心类不多，整体类结构如下图(去掉了一些具体的实现类和异常类):</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/gson%E7%B1%BB%E5%9B%BE.jpg?x-oss-process=style/black" alt="gson类图" /></p>

<p>主要分以下几块：</p>

<ul>
<li><strong>JsonElement(以及子类)</strong> 代表了Json的中的几种类型：<code>对象</code>，<code>数组</code>，<code>基本类型</code>(在Java中主包括: String, Java基本类型以及基本类型的包装类)，<code>Null</code></li>
<li><strong>JsonReader和JsonWriter</strong> 封装了Gson读取和输出Json数据的过程。</li>
<li><strong>TypeAdapter和TypeAdapterFactory</strong> Gson的核心部分，通过TypeAdapter将具体类型的转换细节封装起来，使Gson有良好的灵活性和扩展性。</li>
<li><strong>JsonSerializer和JsonDeserializer</strong> 这里两个接口允许你自定义某个类型的序列化逻辑。不过现在推荐使用<strong>TypeAdapter</strong>的方式</li>
</ul>


<p>可以看到Gson的核心逻辑是很简单的<code>^_^</code>。</p>

<p>第一篇首先通过简单的描述下 <code>new Gson().toJson(new A())</code> 流程来说明下Gson的核心实现，后续再继续深入其他的细节。</p>

<p><img src="https://0bb6ac2.oss-cn-hongkong.aliyuncs.com/image/ARTS-gson-%E6%B5%81%E7%A8%8B.png?x-oss-process=style/black" alt="流程" /></p>

<h3>创建Gson对象</h3>

<p>最简单的用法中创建<code>Gson</code>对象: <code>Gson gson = new Gson()</code>，但是实际上内部做的事情远比看上去的多：</p>

<pre><code class="Java">
public Gson() {
    this(Excluder.DEFAULT, FieldNamingPolicy.IDENTITY,
        Collections.&lt;Type, InstanceCreator&lt;?&gt;&gt;emptyMap(), DEFAULT_SERIALIZE_NULLS,
        DEFAULT_COMPLEX_MAP_KEYS, DEFAULT_JSON_NON_EXECUTABLE, DEFAULT_ESCAPE_HTML,
        DEFAULT_PRETTY_PRINT, DEFAULT_LENIENT, DEFAULT_SPECIALIZE_FLOAT_VALUES,
        LongSerializationPolicy.DEFAULT, null, DateFormat.DEFAULT, DateFormat.DEFAULT,
        Collections.&lt;TypeAdapterFactory&gt;emptyList(), Collections.&lt;TypeAdapterFactory&gt;emptyList(),
        Collections.&lt;TypeAdapterFactory&gt;emptyList());
  }

//实际的
Gson(Excluder excluder, FieldNamingStrategy fieldNamingStrategy,
      Map&lt;Type, InstanceCreator&lt;?&gt;&gt; instanceCreators, boolean serializeNulls,
      boolean complexMapKeySerialization, boolean generateNonExecutableGson, boolean htmlSafe,
      boolean prettyPrinting, boolean lenient, boolean serializeSpecialFloatingPointValues,
      LongSerializationPolicy longSerializationPolicy, String datePattern, int dateStyle,
      int timeStyle, List&lt;TypeAdapterFactory&gt; builderFactories,
      List&lt;TypeAdapterFactory&gt; builderHierarchyFactories,
      List&lt;TypeAdapterFactory&gt; factoriesToBeAdded) {
    // 执行排查的字段和类型  
    this.excluder = excluder;
    // 字段命名机制。比如是 aName 后者是 a_name
    this.fieldNamingStrategy = fieldNamingStrategy;
    // Gson默认是需要有不带参数的构造函数，但是碰到没有无参数构造函数且没有权限修改第三方类的情况下，可以通过InstanceCreator来解决
    this.instanceCreators = instanceCreators;
    // 通过构造器创建指定的类型
    this.constructorConstructor = new ConstructorConstructor(instanceCreators);
    // 是否序列化null
    this.serializeNulls = serializeNulls;
    this.complexMapKeySerialization = complexMapKeySerialization;
    // 是否输出不可执行的json。如果为true，会在开头先输出一个")]}'\n"
    this.generateNonExecutableJson = generateNonExecutableGson;
    // 是否输出Html安全的JSON. 如果为true，在输出前会转义 &lt; &gt; &amp; =
    this.htmlSafe = htmlSafe;
    this.prettyPrinting = prettyPrinting;
    // 是否使用宽松的模式。如果为true，则会忽略某些限制
    this.lenient = lenient;
    this.serializeSpecialFloatingPointValues = serializeSpecialFloatingPointValues;
    // 对long和Long类型的序列化策略
    this.longSerializationPolicy = longSerializationPolicy;
    // 日期的模式
    this.datePattern = datePattern;
    this.dateStyle = dateStyle;
    this.timeStyle = timeStyle;
    // 创建TypeAdapter的工厂类
    this.builderFactories = builderFactories;
    this.builderHierarchyFactories = builderHierarchyFactories;

    List&lt;TypeAdapterFactory&gt; factories = new ArrayList&lt;TypeAdapterFactory&gt;();

    // built-in type adapters that cannot be overridden
    factories.add(TypeAdapters.JSON_ELEMENT_FACTORY);
    factories.add(ObjectTypeAdapter.FACTORY);

    // the excluder must precede all adapters that handle user-defined types
    factories.add(excluder);

    // users' type adapters
    factories.addAll(factoriesToBeAdded);

    // type adapters for basic platform types
    factories.add(TypeAdapters.STRING_FACTORY);
    factories.add(TypeAdapters.INTEGER_FACTORY);
    factories.add(TypeAdapters.BOOLEAN_FACTORY);
    factories.add(TypeAdapters.BYTE_FACTORY);
    factories.add(TypeAdapters.SHORT_FACTORY);
    TypeAdapter&lt;Number&gt; longAdapter = longAdapter(longSerializationPolicy);
    factories.add(TypeAdapters.newFactory(long.class, Long.class, longAdapter));
    factories.add(TypeAdapters.newFactory(double.class, Double.class,
            doubleAdapter(serializeSpecialFloatingPointValues)));
    factories.add(TypeAdapters.newFactory(float.class, Float.class,
            floatAdapter(serializeSpecialFloatingPointValues)));
    factories.add(TypeAdapters.NUMBER_FACTORY);
    factories.add(TypeAdapters.ATOMIC_INTEGER_FACTORY);
    factories.add(TypeAdapters.ATOMIC_BOOLEAN_FACTORY);
    factories.add(TypeAdapters.newFactory(AtomicLong.class, atomicLongAdapter(longAdapter)));
    factories.add(TypeAdapters.newFactory(AtomicLongArray.class, atomicLongArrayAdapter(longAdapter)));
    factories.add(TypeAdapters.ATOMIC_INTEGER_ARRAY_FACTORY);
    factories.add(TypeAdapters.CHARACTER_FACTORY);
    factories.add(TypeAdapters.STRING_BUILDER_FACTORY);
    factories.add(TypeAdapters.STRING_BUFFER_FACTORY);
    factories.add(TypeAdapters.newFactory(BigDecimal.class, TypeAdapters.BIG_DECIMAL));
    factories.add(TypeAdapters.newFactory(BigInteger.class, TypeAdapters.BIG_INTEGER));
    factories.add(TypeAdapters.URL_FACTORY);
    factories.add(TypeAdapters.URI_FACTORY);
    factories.add(TypeAdapters.UUID_FACTORY);
    factories.add(TypeAdapters.CURRENCY_FACTORY);
    factories.add(TypeAdapters.LOCALE_FACTORY);
    factories.add(TypeAdapters.INET_ADDRESS_FACTORY);
    factories.add(TypeAdapters.BIT_SET_FACTORY);
    factories.add(DateTypeAdapter.FACTORY);
    factories.add(TypeAdapters.CALENDAR_FACTORY);
    factories.add(TimeTypeAdapter.FACTORY);
    factories.add(SqlDateTypeAdapter.FACTORY);
    factories.add(TypeAdapters.TIMESTAMP_FACTORY);
    factories.add(ArrayTypeAdapter.FACTORY);
    factories.add(TypeAdapters.CLASS_FACTORY);

    // type adapters for composite and user-defined types
    factories.add(new CollectionTypeAdapterFactory(constructorConstructor));
    factories.add(new MapTypeAdapterFactory(constructorConstructor, complexMapKeySerialization));
    this.jsonAdapterFactory = new JsonAdapterAnnotationTypeAdapterFactory(constructorConstructor);
    factories.add(jsonAdapterFactory);
    factories.add(TypeAdapters.ENUM_FACTORY);

    //在通常情况下，我们自定义的类型会通过这个 ReflectiveTypeAdapter 来做转换。
    factories.add(new ReflectiveTypeAdapterFactory(
        constructorConstructor, fieldNamingStrategy, excluder, jsonAdapterFactory));

    this.factories = Collections.unmodifiableList(factories);
  }
</code></pre>

<p>可以看到<code>Gson</code>提供了很多的开关和扩展点来定制行为。</p>

<h3>创建Reader/Writer</h3>

<p>创建JsonReader 和 JsonWriter。这个会根据Json标准来读取和输出json数据，具体的细节本文先不深入研究，下篇文章深入研究下。</p>

<pre><code class="Java">  public JsonWriter newJsonWriter(Writer writer) throws IOException {
    if (generateNonExecutableJson) {
      writer.write(JSON_NON_EXECUTABLE_PREFIX);
    }
    JsonWriter jsonWriter = new JsonWriter(writer);
    if (prettyPrinting) {
      jsonWriter.setIndent("  ");
    }
    jsonWriter.setSerializeNulls(serializeNulls);
    return jsonWriter;
  }

  public JsonReader newJsonReader(Reader reader) {
    JsonReader jsonReader = new JsonReader(reader);
    jsonReader.setLenient(lenient);
    return jsonReader;
  }
</code></pre>

<h3>获取类型对应的TypeAdapter</h3>

<pre><code class="Java">  public &lt;T&gt; TypeAdapter&lt;T&gt; getAdapter(TypeToken&lt;T&gt; type) {
    // 首先从本地内存获取
    TypeAdapter&lt;?&gt; cached = typeTokenCache.get(type == null ? NULL_KEY_SURROGATE : type);
    if (cached != null) {
      return (TypeAdapter&lt;T&gt;) cached;
    }

    // 接着从ThreadLocal中获取
    Map&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt; threadCalls = calls.get();
    boolean requiresThreadLocalCleanup = false;
    if (threadCalls == null) {
      threadCalls = new HashMap&lt;TypeToken&lt;?&gt;, FutureTypeAdapter&lt;?&gt;&gt;();
      calls.set(threadCalls);
      requiresThreadLocalCleanup = true;
    }

    // the key and value type parameters always agree
    FutureTypeAdapter&lt;T&gt; ongoingCall = (FutureTypeAdapter&lt;T&gt;) threadCalls.get(type);
    if (ongoingCall != null) {
      return ongoingCall;
    }

    try { 
      FutureTypeAdapter&lt;T&gt; call = new FutureTypeAdapter&lt;T&gt;();
      threadCalls.put(type, call);

      // 循环左右的 TypeAdapterFactory 找到可以处理当前类型的第一个 Factory
      for (TypeAdapterFactory factory : factories) {
        TypeAdapter&lt;T&gt; candidate = factory.create(this, type);
        if (candidate != null) {
          call.setDelegate(candidate);
          typeTokenCache.put(type, candidate);
          return candidate;
        }
      }
      throw new IllegalArgumentException("GSON (" + GsonBuildConfig.VERSION + ") cannot handle " + type);
    } finally {
      threadCalls.remove(type);

      if (requiresThreadLocalCleanup) {
        calls.remove();
      }
    }
  }
</code></pre>

<p>前面提到在没有特定指定的情况下，自定义的类型都是由<code>ReflectiveTypeAdapterFactory</code>来处理的，让我们来看下<code>ReflectiveTypeAdapterFactory</code>的<code>create</code>方法。</p>

<pre><code class="Java">
public &lt;T&gt; TypeAdapter&lt;T&gt; create(Gson gson, final TypeToken&lt;T&gt; type) {
    Class&lt;? super T&gt; raw = type.getRawType();

    if (!Object.class.isAssignableFrom(raw)) {
      // 到这里的话说明是基本类型
      return null; 
    }

    // 为指定类型创建一个构造器工厂。 这里会优先使用前面提到的 InstanceCreator.
    ObjectConstructor&lt;T&gt; constructor = constructorConstructor.get(type);

    // 为类型的每个字段映射成内部的 BoundField对象。
    return new Adapter&lt;T&gt;(constructor, getBoundFields(gson, type, raw));
  }
</code></pre>

<p><code>BoundField</code> 是一个重要的对象，定义了一个字段该如何读取和输出。来看下它的定义：</p>

<pre><code class="Java">
  static abstract class BoundField {
    final String name;
    final boolean serialized;
    final boolean deserialized;

    protected BoundField(String name, boolean serialized, boolean deserialized) {
      this.name = name;
      this.serialized = serialized;
      this.deserialized = deserialized;
    }

    // 判断一个字段是否可写
    abstract boolean writeField(Object value) throws IOException, IllegalAccessException;
    // 将字段写入到writer
    abstract void write(JsonWriter writer, Object value) throws IOException, IllegalAccessException;
    // 从输入中读取字段的值
    abstract void read(JsonReader reader, Object value) throws IOException, IllegalAccessException;
  }
</code></pre>

<p>来看看<code>BoundField</code>是如何构建出来的：</p>

<pre><code class="Java">private Map&lt;String, BoundField&gt; getBoundFields(Gson context, TypeToken&lt;?&gt; type, Class&lt;?&gt; raw) {
    Map&lt;String, BoundField&gt; result = new LinkedHashMap&lt;String, BoundField&gt;();

    // 如果是接口直接返回
    if (raw.isInterface()) {
      return result;
    }

    Type declaredType = type.getType();
    while (raw != Object.class) {
      // 获取当前类型声明的字段
      Field[] fields = raw.getDeclaredFields();
      for (Field field : fields) {

        //判断字段是否需要序列化
        boolean serialize = excludeField(field, true);
        boolean deserialize = excludeField(field, false);
        if (!serialize &amp;&amp; !deserialize) {
          continue;
        }
        accessor.makeAccessible(field);
        Type fieldType = $Gson$Types.resolve(type.getType(), raw, field.getGenericType());
        List&lt;String&gt; fieldNames = getFieldNames(field);
        BoundField previous = null;
        for (int i = 0, size = fieldNames.size(); i &lt; size; ++i) {
          String name = fieldNames.get(i);
          if (i != 0) serialize = false; // only serialize the default name
          BoundField boundField = createBoundField(context, field, name,
              TypeToken.get(fieldType), serialize, deserialize);
          BoundField replaced = result.put(name, boundField);
          if (previous == null) previous = replaced;
        }
        if (previous != null) {
          throw new IllegalArgumentException(declaredType
              + " declares multiple JSON fields named " + previous.name);
        }
      }

      //继续处理父类
      type = TypeToken.get($Gson$Types.resolve(type.getType(), raw, raw.getGenericSuperclass()));
      raw = type.getRawType();
    }
    return result;
  }

private ReflectiveTypeAdapterFactory.BoundField createBoundField(
      final Gson context, final Field field, final String name,
      final TypeToken&lt;?&gt; fieldType, boolean serialize, boolean deserialize) {

    final boolean isPrimitive = Primitives.isPrimitive(fieldType.getRawType());

    // 为字段关联TypeAdapter. 首先看是否有通过注解特别指定的，没有的话通过getAdapter来创建
    JsonAdapter annotation = field.getAnnotation(JsonAdapter.class);
    TypeAdapter&lt;?&gt; mapped = null;
    if (annotation != null) {
      mapped = jsonAdapterFactory.getTypeAdapter(
          constructorConstructor, context, fieldType, annotation);
    }
    final boolean jsonAdapterPresent = mapped != null;
    if (mapped == null) mapped = context.getAdapter(fieldType);

    final TypeAdapter&lt;?&gt; typeAdapter = mapped;
    return new ReflectiveTypeAdapterFactory.BoundField(name, serialize, deserialize) {

      @Override void write(JsonWriter writer, Object value)
          throws IOException, IllegalAccessException {
        Object fieldValue = field.get(value);
        TypeAdapter t = jsonAdapterPresent ? typeAdapter
            : new TypeAdapterRuntimeTypeWrapper(context, typeAdapter, fieldType.getType());
        t.write(writer, fieldValue);
      }
      @Override void read(JsonReader reader, Object value)
          throws IOException, IllegalAccessException {
        Object fieldValue = typeAdapter.read(reader);
        if (fieldValue != null || !isPrimitive) {
          field.set(value, fieldValue);
        }
      }
      @Override public boolean writeField(Object value) throws IOException, IllegalAccessException {
        if (!serialized) return false;
        Object fieldValue = field.get(value);
        return fieldValue != value; 
      }
    };
  }
</code></pre>

<p>至此，已经解析好了该类以及父类的所有字段。接下来看下是TypeAdapter是怎么执行转换的。</p>

<h3>TypaAdapter执行转换</h3>

<p>这块从<code>ReflectiveTypeAdapterFactory</code>的实现来看就比较简单了，通过对每个解析好的<code>BoundField</code> 调用它的<code>read</code>和<code>write</code>方法即可。</p>

<pre><code class="Java">public void toJson(Object src, Type typeOfSrc, JsonWriter writer) throws JsonIOException {
    TypeAdapter&lt;?&gt; adapter = getAdapter(TypeToken.get(typeOfSrc));
    boolean oldLenient = writer.isLenient();
    writer.setLenient(true);
    boolean oldHtmlSafe = writer.isHtmlSafe();
    writer.setHtmlSafe(htmlSafe);
    boolean oldSerializeNulls = writer.getSerializeNulls();
    writer.setSerializeNulls(serializeNulls);
    try {
      ((TypeAdapter&lt;Object&gt;) adapter).write(writer, src);
    } catch (IOException e) {
      throw new JsonIOException(e);
    } catch (AssertionError e) {
      throw error;
    } finally {
      writer.setLenient(oldLenient);
      writer.setHtmlSafe(oldHtmlSafe);
      writer.setSerializeNulls(oldSerializeNulls);
    }
  }

public &lt;T&gt; T fromJson(JsonReader reader, Type typeOfT) throws JsonIOException, JsonSyntaxException {
    boolean isEmpty = true;
    boolean oldLenient = reader.isLenient();
    reader.setLenient(true);
    try {
      reader.peek();
      isEmpty = false;
      TypeToken&lt;T&gt; typeToken = (TypeToken&lt;T&gt;) TypeToken.get(typeOfT);
      TypeAdapter&lt;T&gt; typeAdapter = getAdapter(typeToken);
      T object = typeAdapter.read(reader);
      return object;
    } catch (EOFException e) {
      throw new JsonSyntaxException(e);
    } catch (IllegalStateException e) {
      throw new JsonSyntaxException(e);
    } catch (IOException e) {
      throw new JsonSyntaxException(e);
    } catch (AssertionError e) {
      throw error;
    } finally {
      reader.setLenient(oldLenient);
    }
  }
</code></pre>

<h3>总结</h3>

<p>从代码中可以看到，Gson针对所有的类型细节的处理都抽象封装到了TypeAdapter中，这样使得主流程可以不用关心转换细节，同时也为用户扩展提供了无限的可能。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Arts-week-four]]></title>
    <link href="http://linuxlsx.github.io/blog/2019/04/14/arts-week-four/"/>
    <updated>2019-04-14T00:06:02+08:00</updated>
    <id>http://linuxlsx.github.io/blog/2019/04/14/arts-week-four</id>
    <content type="html"><![CDATA[<h2>Algorithm</h2>

<p>本周完成的算法题: <a href="https://leetcode.com/problems/first-missing-positive/">First Missing Positive</a></p>

<p>本题非独立完成，根据<a href="https://stackoverflow.com/questions/1586858/find-the-smallest-integer-not-in-a-list">Find the Smallest Integer Not in a List</a> 参考实现</p>

<p>题目要求:</p>

<pre><code class="Text">给定一个没有排序的Int数组，做到数组中不存在的最小正整数。

Note:
1. 算法复杂度必须是O(n)
2. 使用常数个额外的空间
</code></pre>

<!-- more -->


<p><strong>分析:</strong></p>

<p>这个题的解法很巧妙，通过利用数组的下标和其对应的值做比较就可以确定确实的最小正整数。算法运行时间至多为<code>O(3N)</code>，而且只需要一个额外的Int变量。</p>

<pre><code class="Java">    public int firstMissingPositive(int[] nums) {

        int p;
        //第一个O(N)
        for (int i = 0; i &lt; nums.length; i++) {
            p = nums[i];

            //为什么这里也只会最多执行O(N)次
            //因为在条件中会判断 nums[p - 1] != p，然后在循环体里面会设置 nums[p - 1] = p
            //那么每执行一次赋值，就会减少一个可赋值的位置。
            //就可以保证至多执行N次
            while (p &gt; 0 &amp;&amp; p &lt; nums.length &amp;&amp; nums[p - 1] != p) {
                nums[i] = nums[p - 1];
                nums[p - 1] = p;
                p = nums[i];
            }
        }

        //第三个O(N)
        for (int i = 0; i &lt; nums.length; i++) {
            if (i + 1 != nums[i]) {
                return i + 1;
            }
        }

        ////只有数组是 [1,2,3,4] 这种情况才会走到这里
        return nums.length + 1;
    }
</code></pre>

<h2>Review</h2>

<p>本周Review <a href="https://medium.com/s/story/reflections-on-clean-code-8c9b683277ca">What is Clean Code?</a></p>

<p>本文是作者对于《Clean Code》一书的总结，从他的视角来看，可以归纳总结为三点:</p>

<ul>
<li><strong>工匠精神(Craftsmanship Matters)</strong> 可以<code>Works</code>的代码并不能说已经<code>done</code>，粗制滥造的代码会比你想象的快的出现问题。(Poorly crafted code frays at the edge much faster than you might expect.)</li>
<li><strong>今天的额外努力会减少明天的痛苦(Extra Effort Today Saves Pain Tomorrow)</strong> 就跟买衣服，鞋子一样，好的鞋子可能在价格上会高些，但是它可以穿两年、三年甚至十年，那么长远来看，这个价格就不高了。高质量的代码也是一样的，可以在开始时会困难些，但是会降低未来的技术债和维护成本。</li>
<li><strong>你写的代码不仅仅是你的(Your Code Is Not Your Own)</strong> 过于聪明的把戏，黑科技和编程技巧只是当时作者的乐趣(Overly clever tricks, hacks, and sleights of programmatic hand are only fun for the author.)，但是会给后续的维护者(也可能是你自己)带来无数的麻烦。</li>
</ul>


<p>对于什么是<code>Clean Code</code>，也总结了一下几点:</p>

<ul>
<li>Clean code is simple。虽然可能在算法或者系统层级上复杂，但是实现上要简单。少用比较冷门的技巧。</li>
<li>Clean code is readable。在命名规范、缩进、结构和流程要有良好的设计，虽然显得有些古板但是会减少后来着的理解难度。</li>
<li>Clean code is considerate。Clean code 要为后续的读者考虑，可以假设他们也是拥有同样背景和经验的人。</li>
<li>Clean code is tested。Clean code 要有充分的测试</li>
<li>Clean code is practiced。Clean code 需要多练习</li>
<li>Clean code is relentlessly refactored。Clean code 需要经常重构</li>
<li>Clean code is SOLID。遵循<a href="https://medium.com/@severinperez/writing-flexible-code-with-the-single-responsibility-principle-b71c4f3f883f">Solid 原则</a></li>
</ul>


<p>结合自身的实际来说，我觉得最重要的是 <code>tested</code> 和 <code>readable</code>。只有测试过的代码才能保证联调、测试和线上运行的时候不出现问题。另外很少有情况是你一直维护一套代码，所以让代码有良好的可读性会对后来者有很大帮助，同时自己也会有成就感。</p>

<h2>Tip</h2>

<p>本周分享一个线上排查问题时需要将日志内容按照不同的要求写到不同的文件。</p>

<pre><code class="Bash">awk -F',' '{if ($2 ==1) print $1 &gt; "down.txt"; else if($2 == 6) print $1 &gt; "delete.txt"; else print $1 &gt; "other.txt"}' xxx.log
</code></pre>

<h2>Share</h2>

<p><strong>主要负载均衡算法</strong></p>

<p>分布式场景下client 访问 server 的策略有很多，典型的有 轮询，随机，Hash，最少连接，响应速度，加权算法等。</p>

<p><strong>轮询算法</strong>：将所有请求，依次分发到每台服务器上。缺点: 所有机器的请求一样， 处理能力慢的机器压力变大。
<strong>随机算法</strong>：请求随机分配到各个服务器。简单。缺点：理论上所有机器的请求一样，处理能力慢的机器压力变大。
<strong>最少连接</strong>：将请求分配到连接数最少的服务器（目前处理请求最少的服务器）。优点：根据服务器当前的请求处理情况，动态分配。缺点: 实现比较复杂，需要感知服务器的状态
<strong>Hash</strong> : 根据IP地址进行Hash计算，得到IP地址。优点： 将来自同一IP地址的请求，同一会话期内，转发到相同的服务器；实现会话粘滞。缺点：机器变化时会需要重新计算，最坏情况下所有请求都需要变化。
加权算法：在轮询，随机，最少链接，Hash’等算法的基础上，通过加权的方式，进行负载服务器分配。优点：根据权重，调节转发服务器的请求数目；缺点：使用相对复杂；
<strong>一致性 Hash</strong> ：一致性Hash 的实现是将 node 映射到一个 固定长度的圆环上，请求计算出Hash值按照预定的规则（一般是按照顺时钟方向）落到最近的服务器上。这样当出现机器的变化时，只会影响受影响机器与该机器反方向最近有效服务器之间的节点。其他的节点不会收到影响。当新增node时，只需要计算出该node的hash地址，然后将node 添加到 环的对应位置，只有新node 与后一个node之间的数据会受到影响。这种设计下的扩展性和容错性都比较好。比较不好的问题是在服务器比较少的情况下容易出现负责不均和的情况下。</p>

<p>一致性Hash 的实现是将 node 映射到一个 固定长度的圆环上，并在有效节点中引入多个虚节点（每个虚节点都是有效的的一个影子）。这个所有请求能够均匀的落到node上。</p>

<p>在现在的一致性hash实现上，Google 的研究人员发现在多次负载均衡过后不能够获得最优的负载均衡的效果，开发一个带负载因子(a)的一致性hash算法。</p>

<p>这个算法的能够保证一致性和负责的均衡性（所有的机器的load 都是平均load 的 （1+a）倍）。当出现机器的新增和删除后，通过节点移动的操作重新使得负载重新达到均衡，而这个移动的次数只跟负载因子(a)相关，而与实际节点数的多少无关。每个新增或者删除节点的动作只会导致  O(1/a2) 次其他节点的移动。</p>

<p>更多一致性Hash参考 : <a href="https://juejin.im/post/5b8f93576fb9a05d11175b8d">一致性Hash在负载均衡中的应用</a> </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Arts-week-three]]></title>
    <link href="http://linuxlsx.github.io/blog/2019/04/08/arts-week-three/"/>
    <updated>2019-04-08T00:00:13+08:00</updated>
    <id>http://linuxlsx.github.io/blog/2019/04/08/arts-week-three</id>
    <content type="html"><![CDATA[<h1>Algorithm</h1>

<p>本周完成的算法题: <a href="https://leetcode.com/problems/combination-sum/">Combination Sum</a></p>

<p>题目要求:</p>

<pre><code>给定一个不重复的数组(candidates)，以及一个目标(target)，从candidates找到所有的不重复的组合使得他们的和等于target。

Note:
1. 所有的数字都是正整数
2. 同一个数字可以无限次的重复使用
</code></pre>

<!--more-->


<p><strong>分析:</strong></p>

<p>这题可以通过回溯法来解决，通过递归下降多次遍历数组来寻找满足条件的组合。</p>

<pre><code class="Java">public class CombinationSum {

    public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) {

        List&lt;List&lt;Integer&gt;&gt; lists = new ArrayList&lt;&gt;(8);

        Stack&lt;Integer&gt; stack = new Stack&lt;&gt;();
        stack.ensureCapacity(candidates.length);
        calc(candidates, target, 0,new Stack&lt;&gt;(), lists);

        return lists;
    }

    /**
     * 递归下降深度优先遍历。 算法复杂度 O((n^2 + n) / 2)
     * 最终结果是 6ms  beats 92%
     *
     * 分析了排在更前面的实现，发现主要的差异就是他们用List替代了Stack
     * 来保存中间状态，这样可以减少一个subList的操作。基本上可以到2ms
     *
     * @param candidates
     * @param target
     * @param start
     * @param stack         需要使用栈来保存中间状态。
     * @param lists
     */
    private void calc(int[] candidates, int target, int start, Stack&lt;Integer&gt; stack, List&lt;List&lt;Integer&gt;&gt; lists){

        for (int i = start; i &lt; candidates.length; i++) {

            if(candidates[i] &gt; target){
                continue;
            }

            stack.push(candidates[i]);
            if(candidates[i] == target){
                List&lt;Integer&gt; l = new ArrayList&lt;&gt;(stack.size());
                l.addAll(stack.subList(0, stack.size()));
                lists.add(l);
            }else if(candidates[i] &lt; target) {
                calc(candidates, target - candidates[i], i ,stack , lists);
            }
            stack.pop();
        }
    }
}    
</code></pre>

<h1>Review</h1>

<p>本周Review <a href="https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977">Building and Scaling Data Lineage at Netflix to Improve Data Infrastructure Reliability, and Efficiency</a></p>

<p>本文是Netflix的一篇技术文章。讲述了Netflix内部是如何构建一个数据链路关系系统的。</p>

<p>公司从小到大的过程中，一定会伴随着数据越来越多，数据链路越来越复杂。很快就没有人能够了解数据全貌了。文章开篇提了三个场景:</p>

<ul>
<li>假设你是一个决策者，当你要根据数据看报做一个关键决策时，是否可以自己去验证下看报背后的数据到底是什么</li>
<li>假设你是一个开发者，当你决定要修改你提供的服务的数据结构时，是否可以知道哪些下游会受到影响</li>
<li>假设你现在负责平台的可靠性，你的任务是主动监测上游任务的问题，提前给数据作业owner报警。你要设计一个SLA预警系统，需要用到上下游数据依赖以及历史状态数据。<code>(Finally, imagine yourself in the role of a data platform reliability engineer tasked with providing advanced lead time to data pipeline (ETL) owners by proactively identifying issues upstream to their ETL jobs. You are designing a learning system to forecast Service Level Agreement (SLA) violations and would want to factor in all upstream dependencies and corresponding historical states)</code></li>
</ul>


<p>要满足以上的需求，就需要有一个 <code>complete and accurate data lineage system</code>。</p>

<p>自由和责任是Netflix内部文化的重要部分。核心思想是你可以自由的选择实现方式，但是要为其负责。同样这样的自由也会导致公司内部技术栈的多样性，同样也会带来更多的负责度。作者就面临这样的问题。以下是Netflix 的一个<code>Data Landscape</code>。</p>

<p><img src="https://cdn-images-1.medium.com/max/1400/0*gYI3uCywVhSrcoRo" alt="Data Landscape" /></p>

<p>为了满足Netflix内部多样化的数据，<code>Data Lineage</code>有以下的几个设计原则：</p>

<ul>
<li><code>Ensure data integrity</code> 数据完整性。 需要精确完整的保存数据关系来建立用户的信心。一个不完全可行系统带来伤害可能多过好处。</li>
<li><code>Enable seamless intergration</code> 无缝接入。需要能够满足新的数据工具快速接入。</li>
<li><code>Design a flexible data model</code> 灵活的数据结构。使用一个通用灵活的数据模型来表示不同的数据来源。</li>
</ul>


<p>最终的系统实现图如下：</p>

<p><img src="https://cdn-images-1.medium.com/max/2600/0*Xp1KHPFm1R7GZGAI" alt="系统实现图" /></p>

<p>解释下这个图：</p>

<p>首先左侧是数据接入层，每个业务系统都有它自己独立数据处理逻辑，独立的数据模型。所以在使用之前需要统一进行转换。</p>

<p>中间一层就是数据转换层，转换层将所有收集上来的数据进行转换，统一用点和边的图模式表达。数据转换完毕后会将数据存储到图数据库中。</p>

<p>右侧就是数据存储和服务层。在数据之上建立了的REST服务层，对外提供数据服务。</p>

<h1>Tip</h1>

<p>在Java中很多情况需要对一些特殊字符做一些转义。那么一般的写法就是为罗列出所有的需要进行替换的字符，然后不停的调用replace。比如像下面的代码：</p>

<pre><code class="Java">keyword.replace("\\", "\\\\").replace("*", "\\*")
                .replace("+", "\\+").replace("|", "\\|")
                .replace("{", "\\{").replace("}", "\\}")
                .replace("(", "\\(").replace(")", "\\)")
                .replace("^", "\\^").replace("$", "\\$")
                .replace("[", "\\[").replace("]", "\\]")
                .replace("?", "\\?").replace(",", "\\,")
                .replace(".", "\\.").replace("&amp;", "\\&amp;")
                .replace("-", "\\-");
</code></pre>

<p>这样写虽然能够完成需求，但是不太优雅。在Java的正则表达式中实际是可以支持类似<code>$0 $1</code>这样的占位符的。那么像<code>.replace("[", "\\[")</code> 可以改写成 <code>.replace("[", "\\$0")</code>。</p>

<p>在这个实现中，<code>$0</code> 代表的是整个正则的匹配结果，<code>$1 $2...$n</code> 代表的是从1-n的捕获组的内容。</p>

<p>具体的捕获组的介绍可以参考：<a href="https://blog.csdn.net/just4you/article/details/70767928">正则表达式的捕获组(capture group)在Java中的使用</a></p>

<p>上面的语句改成类似形式的案例：</p>

<pre><code class="Java">String s = "\\*+|{}^$[]?,.&amp;-";
System.out.println(s);
System.out.println(s.replaceAll("[\\\\\\*\\+\\|\\{\\}\\(\\)\\^\\$\\[\\]\\?\\,\\.\\&amp;\\-]", "\\\\$0"));

//输出
\*+|{}^$[]?,.&amp;-
\\\*\+\|\{\}\^\$\[\]\?\,\.\&amp;\-
</code></pre>

<h1>Share</h1>

<p><a href="http://linuxlsx.top/blog/2019/04/07/la-ji-hui-shou-de-suan-fa-yu-shi-xian-du-shu-bi-ji-fu-zhi-suan-fa/">GC算法-复制算法</a></p>
]]></content>
  </entry>
  
</feed>
